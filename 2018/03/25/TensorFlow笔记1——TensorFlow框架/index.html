<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine Learning,TensorFlow," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="基于TensorFlow搭建神经网络（Neural Network）的主要流程有：  用张量表示数据 用计算图搭建NN 用Session执行计算图 优化线上权重参数 得到模型">
<meta name="keywords" content="Machine Learning,TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow笔记1——TensorFlow框架">
<meta property="og:url" content="http://victorzhong.github.io/2018/03/25/TensorFlow笔记1——TensorFlow框架/index.html">
<meta property="og:site_name" content="Victor&#39;s Sketch">
<meta property="og:description" content="基于TensorFlow搭建神经网络（Neural Network）的主要流程有：  用张量表示数据 用计算图搭建NN 用Session执行计算图 优化线上权重参数 得到模型">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://victorzhong.github.io/uploads/2018032501.png">
<meta property="og:image" content="http://victorzhong.github.io/uploads/2018032502.png">
<meta property="og:image" content="http://victorzhong.github.io/uploads/2018032503.png">
<meta property="og:image" content="http://victorzhong.github.io/uploads/2018032504.png">
<meta property="og:updated_time" content="2018-09-06T14:14:27.717Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow笔记1——TensorFlow框架">
<meta name="twitter:description" content="基于TensorFlow搭建神经网络（Neural Network）的主要流程有：  用张量表示数据 用计算图搭建NN 用Session执行计算图 优化线上权重参数 得到模型">
<meta name="twitter:image" content="http://victorzhong.github.io/uploads/2018032501.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://victorzhong.github.io/2018/03/25/TensorFlow笔记1——TensorFlow框架/"/>





  <title>TensorFlow笔记1——TensorFlow框架 | Victor's Sketch</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Victor's Sketch</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://victorzhong.github.io/2018/03/25/TensorFlow笔记1——TensorFlow框架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Chung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Victor's Sketch">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">TensorFlow笔记1——TensorFlow框架</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-25T12:58:00+08:00">
                2018-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>基于TensorFlow搭建神经网络（Neural Network）的主要流程有：</p>
<ol>
<li>用张量表示数据</li>
<li>用计算图搭建NN</li>
<li>用Session执行计算图</li>
<li>优化线上权重参数</li>
<li>得到模型</li>
</ol>
<a id="more"></a>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="张量-Tensor-：多维数组-列表-，用“阶”表示其维度。"><a href="#张量-Tensor-：多维数组-列表-，用“阶”表示其维度。" class="headerlink" title="张量(Tensor)：多维数组(列表)，用“阶”表示其维度。"></a>张量(Tensor)：多维数组(列表)，用“阶”表示其维度。</h3><p>0阶张量称为<strong>标量</strong>，表示一个常数，如S=123。<br>1阶张量称为<strong>向量</strong>，表示一个一维数组，如V=[1,2,3]。<br>2阶张量称为<strong>矩阵</strong>，表示一个二维数组，可以有i行j列个元素，每个元素可由i、j索引到，如M=[[1,2,3],[4,5,6],[7,8,9]]。<br><strong>计算张量的维度</strong> 最简单直观的方法是数最右的方括号]的数量，有多少个就是几阶的。</p>
<h3 id="计算图-Graph-：搭建NN的计算过程，承载一个或多个计算节点（只搭建，不计算）"><a href="#计算图-Graph-：搭建NN的计算过程，承载一个或多个计算节点（只搭建，不计算）" class="headerlink" title="计算图(Graph)：搭建NN的计算过程，承载一个或多个计算节点（只搭建，不计算）"></a>计算图(Graph)：搭建NN的计算过程，承载一个或多个计算节点（只搭建，不计算）</h3><p>神经网络的基本模型是神经元，而神经元的基本模型其实就是乘、加运算，比如下图就是一个神经元。</p>
<p><img src="/uploads/2018032501.png" alt="计算图示例"></p>
<p>$X_1$、$X_2$ 表示输入，$W_1$、$W_2$分别为$X_1$、$X_2$到y的权重，那么<br>$$y=X_1*W_1+X_2*W_2$$<br>用TF实行的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])		<span class="comment">#定义2阶张量[[1.0, 2.0]]</span></span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]])		<span class="comment">#定义2阶张量[[3.0], [4.0]]</span></span><br><span class="line">y = tf.matmul(x, w)					<span class="comment">#实现XW矩阵相乘</span></span><br><span class="line"><span class="keyword">print</span> y</span><br></pre></td></tr></table></figure>
<p>可以看到控制台输出了<code>Tensor(“matmul:0”, shape(1,1), dtype=float32)</code>，其中<code>“matmul:0”</code>是张量名，<code>shape(1,1)</code>表示1行1列，<code>dtype=float32)</code>说明数据类型为float32。<br>这里输出的y张量只是一个计算过程，并不是计算结果。计算结果的获取需要通过Session执行。</p>
<h3 id="会话-Session-：执行计算图的节点。"><a href="#会话-Session-：执行计算图的节点。" class="headerlink" title="会话(Session)：执行计算图的节点。"></a>会话(Session)：执行计算图的节点。</h3><p>在TF中用<code>with</code>结构实现，语法如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	<span class="keyword">print</span> sess.run(y)</span><br></pre></td></tr></table></figure></p>
<p>那对于上述的计算图，用Session计算的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]])</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="keyword">print</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	<span class="keyword">print</span> sess.run(y)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看到计算结果为11。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(“matmul:0”, shape(1,1), dtype=float32)</span><br><span class="line">[[11.]]</span><br></pre></td></tr></table></figure>
<h2 id="NN的参数"><a href="#NN的参数" class="headerlink" title="NN的参数"></a>NN的参数</h2><p>神经网络的参数是指神经元线上的权重Weight，一般用W表示，一般会先随机生成。TF中调用<code>tf.Variable()</code>方法给W赋值。<br>TF中常用的生成随机数或数组的函数有：</p>
<table>
<thead>
<tr>
<th>函数</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tf.random_normal()</code></td>
<td>生成正态分布的随机数</td>
</tr>
<tr>
<td><code>tf.truncated_normal()</code></td>
<td>生成去掉过大偏离点的正态分布随机数</td>
</tr>
<tr>
<td><code>tf.random_uniform()</code></td>
<td>生成均匀分布随机数</td>
</tr>
<tr>
<td><code>tf.zeros</code></td>
<td>生成全 0 数组</td>
</tr>
<tr>
<td><code>tf.ones</code></td>
<td>生成全 1 数组</td>
</tr>
<tr>
<td><code>tf.fill</code></td>
<td>生成全定值数组</td>
</tr>
<tr>
<td><code>tf.constant</code></td>
<td>生成直接给定值的数组</td>
</tr>
</tbody>
</table>
<p>例如：</p>
<ul>
<li><code>w=tf.Variable(tf.random_normal([2,3],stddev=2, mean=0, seed=1))</code> 表示生成正态分布随机数，形状2行3列，标准差是 2，均值是 0，随机种子是 1。</li>
<li><code>w=tf.Variable(tf.truncated_normal([2,3],stddev=2, mean=0, seed=1))</code>表示去掉偏离过大的正态分布，也就是如果随机出来的数据偏离平均值超过两个标准差则重新生成。</li>
<li><code>w=random_uniform(shape=7,minval=0,maxval=1,dtype=tf.int32，seed=1)</code> 表示从一个均匀分布[minval, maxval)中随机采样，注意定义域是左闭右开，即:包含minval，不包含 maxval。</li>
<li>除了生成随机数，还可以生成常量。<code>tf.zeros([3,2],int32)</code>表示生成 [[0,0],[0,0],[0,0]];<code>tf.ones([3,2],int32)</code>表示生成[[1,1],[1,1],[1,1]; <code>tf.fill([3,2],6)</code>表示生成[[6,6],[6,6],[6,6]];<code>tf.constant([3,2,1])</code>表示生成[3,2,1]。</li>
</ul>
<p>注意：如果不指定随机数种子，每次生成的随机数将不一致。另外，如果没有特殊要求，标准差、均值、随机种子是可以不填的。</p>
<h2 id="神经网络实现过程"><a href="#神经网络实现过程" class="headerlink" title="神经网络实现过程"></a>神经网络实现过程</h2><p>当知道了张量、计算图、会话、参数后，我们就可以来讨论神经网络具体的实现过程了。主要分为四步：</p>
<ol>
<li>准备数据集，提取特征，作为输入喂给神经网络</li>
<li>从输入到输出搭建 NN 结构，也就是先搭建计算图，再用会话执行得到输出结果。（NN 前向传播算法==&gt;计算输出）</li>
<li>大量特征数据喂给 NN，迭代优化 NN 参数。(NN 反向传播算法==&gt;优化参数训练模型)</li>
<li>使用训练好的模型预测和分类</li>
</ol>
<p>通过第1至3步的循环迭代训练，可以不断优化参数。而参数优化一旦完成，此神经网络便可投入具体的应用了。</p>
<p>很多实际应用中，我们会先使用现有的成熟网络结构，喂入新的数据，训练相应的模型，判断是否能对喂入的从未见过的新数据作出正确响应，再适当更改网络结构，反复迭代，让机器自动训练参数找出最优结构和参数，以固定专用模型。</p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>前向传播就是搭建NN的计算过程，让模型具有推理能力，能针对一组输入给出相应的输出。<br>举个例子：<br>假如生产一批零件，体积为 $X_1$，重量为  $X_2$，体积和重量就是我们选择的特征， 把它们喂入神经网络，当体积和重量这组数据走过神经网络后会得到一个输出。</p>
<p><img src="/uploads/2018032502.png" alt="零件生产计算图"></p>
<p>假如输入的特征值是：体积 0.7，重量 0.5，由以上计算图，隐藏层节点 <span>$a_{11} = X_1 * W_{1,1}^{(1)} + X_2 * W_{2,1}^{(1)} = 0.14 + 0.15 = 0.29$</span><!-- Has MathJax -->，</p>
<p>同理算得节点 <span>$a_{12} = 0.32$</span><!-- Has MathJax --> ， <span>$a_{13} = 0.38$</span><!-- Has MathJax --> ，<br>最终计算得到输出层 Y=-0.015，这便实现了<strong>前向传播过程</strong>。</p>
<p>具体推导过程如下：</p>
<p>1） $X$ 表示输入。</p>
<p>在此例中是1行2列矩阵，表示一次输入一组特征，包含体积和重量两个元素。</p>
<p>2） $W_{前节点编号,后节点编号}^{(层数)}$ 为待优化的权重参数。</p>
<p>对于第一层的W前面有两个节点，后面有三个节点，所以W是个2行3列的矩阵，可以这样表示：<br><span>$$W^{(1)} = 
\begin{bmatrix}
  W_{1,1}^{(1)} &amp; W_{1,2}^{(1)} &amp; W_{1,3}^{(1)} \\
  W_{2,1}^{(1)} &amp; W_{2,2}^{(1)} &amp; W_{2,3}^{(1)} \\
\end{bmatrix}$$</span><!-- Has MathJax --></p>
<p>而对于第二层的W，前面有三个节点，后面有一个节点，所以W是个3行1列的矩阵，表示为：<br><span>$$W^{(2)}=
\begin{bmatrix}
&Tab;W_{1,1}^{(2)}  \\
&Tab;W_{2,1}^{(2)}  \\
&Tab;W_{3,1}^{(2)}  \\
\end{bmatrix}$$</span><!-- Has MathJax --></p>
<p>3） 神经网络的层数指的都是计算层，输入层不是计算层。所以第一层为$a$，是一个1行3列的矩阵：<br><span>$$a=\begin{bmatrix}
       a_{11} &amp; a_{12} &amp; a_{13} \\
       \end{bmatrix}=XW^{(1)}$$</span><!-- Has MathJax --></p>
<p>4) 由以上，$Y=aW^{(2)}$。</p>
<h3 id="前向传播TensorFlow描述"><a href="#前向传播TensorFlow描述" class="headerlink" title="前向传播TensorFlow描述"></a>前向传播TensorFlow描述</h3><p>我们先来一波简单的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.5</span>]])</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"y is:\n"</span>, sess.run(y)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">y is:</span></span><br><span class="line"><span class="string">[[3.0904665]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>以上代码中，随机生成$w_1$和$w_2$时要注意前后节点数，<code>tf.global_variables_initializer()</code>方法对所有全局变量进行初始化，再调用<code>sess.run()</code>进行计算。</p>
<p>在实际应用中，我们可以一次喂入一组或多组输入，让神经网络计算输出 y，可以先用 <code>tf.placeholder()</code> 给输入占位。如果一次喂一组数据 shape 的第一维位置写 1，第二维位置看有几个输入特征；如果一次想喂多组数据，shape 的第一维位置可以写 None 表示先空着，第二维位置写有几个输入特征，这样在 feed_dict中就可以喂入若干组体积重量了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder实现输入定义（sess.run中喂多组数据）</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"y is:\n"</span>, sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>], [<span class="number">0.4</span>, <span class="number">0.5</span>]]&#125;)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">y is:</span></span><br><span class="line"><span class="string">[[3.0904665]</span></span><br><span class="line"><span class="string"> [1.2236414]</span></span><br><span class="line"><span class="string"> [1.7270732]</span></span><br><span class="line"><span class="string"> [2.2305048]]</span></span><br><span class="line"><span class="string">w1:</span></span><br><span class="line"><span class="string">[[-0.8113182   1.4845988   0.06532937]</span></span><br><span class="line"><span class="string"> [-2.4427042   0.0992484   0.5912243 ]]</span></span><br><span class="line"><span class="string">w2:</span></span><br><span class="line"><span class="string">[[-0.8113182 ]</span></span><br><span class="line"><span class="string"> [ 1.4845988 ]</span></span><br><span class="line"><span class="string"> [ 0.06532937]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播的过程是训练、优化模型参数，使 NN 模型在训练数据上的损失函数最小。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>损失函数(loss)</strong>是计算得到的预测值 y 与已知答案 y_ 的差距。常用的计算损失函数的方法有：</p>
<ol>
<li>均方误差MSE(Mean Squared Error)：求前向传播计算结果与已知答案之差的平方的和，再求平均。<span>$MSE(y\_ , y)=\frac {\sum_{i=1}^n(y-y\_)^2}{n}$</span><!-- Has MathJax -->
用TF描述为：<code>loss_mse = tf.reduce_mean(tf.square(y_ - y))</code></li>
<li>自定义损失函数</li>
<li>交差熵CE(Cross Entropy)</li>
</ol>
<h4 id="反向传播的训练方法"><a href="#反向传播的训练方法" class="headerlink" title="反向传播的训练方法"></a>反向传播的训练方法</h4><p><strong>反向传播的训练方法</strong>就是以减小loss 值为优化目标，常用的有梯度下降、momentum 优化器、adam 优化器等优化方法，分别用TF描述为：</p>
<ul>
<li><code>train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</code></li>
<li><code>train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</code></li>
<li><code>train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</code></li>
</ul>
<p>三种优化方法区别如下:<br>1） 梯度下降会使参数沿着梯度的反方向，即总损失减小的方向移动，实现更新参数，如下图所示。</p>
<p><img src="/uploads/2018032503.png" alt="计算图示例"><br>参数的更新公式为：<br><img src="/uploads/2018032504.png" alt="梯度下降"></p>
<p>其中，𝐽(𝜃)为损失函数，𝜃为参数，𝛼为学习率。</p>
<p>2） Momentum优化器在更新参数时，利用了超参数，参数更新公式是：</p>
   <span>$d_i = \beta d_{i-1} + g(\theta_{i-1})$</span><!-- Has MathJax -->
   <span>$\theta_i = \theta_{i-1}-\alpha d_i$</span><!-- Has MathJax -->
<p>其中，𝛼为学习率，超参数为𝛽，𝜃为参数，<span>$g(\theta_{i-1})$</span><!-- Has MathJax -->为损失函数的梯度。</p>
<p>3） Adam优化器是利用自适应学习率的优化算法，Adam 算法和随机梯度下降算法不同的是随机梯度下降算法保持单一的学习率更新所有的参数，学习率在训练过程中并不会改变；而 Adam 算法通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。</p>
<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p><strong>学习率</strong>决定了每次参数更新的幅度。优化器中都需要一个叫做学习率的参数，使用时，如果学习率选择过大会出现震 荡不收敛的情况，如果学习率选择过小，会出现收敛速度慢的情况。我们可以选 个比较小的值填入，比如 0.01、0.001。</p>
<h2 id="搭建NN的八股"><a href="#搭建NN的八股" class="headerlink" title="搭建NN的八股"></a>搭建NN的八股</h2><p>经过前面的梳理，可以总结出搭建NN的四个主要步骤：准备工作、 前向传播、反向传播和循环迭代。TF的框架如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搭建NN的框架</span></span><br><span class="line"><span class="comment"># 0.导入模块，生成模拟数据集;</span></span><br><span class="line"><span class="keyword">import</span> 常量定义 生成数据集</span><br><span class="line"><span class="comment"># 1.前向传播:定义输入、参数和输出 x= y_=</span></span><br><span class="line">x=</span><br><span class="line">y_=</span><br><span class="line">w1= </span><br><span class="line">w2=</span><br><span class="line">a= </span><br><span class="line">y=</span><br><span class="line"><span class="comment"># 2. 反向传播:定义损失函数、反向传播方法</span></span><br><span class="line">loss=</span><br><span class="line">train_step= </span><br><span class="line"><span class="comment"># 3. 生成会话，训练 STEPS 轮</span></span><br><span class="line"><span class="keyword">with</span> tf.session() <span class="keyword">as</span> sess</span><br><span class="line">	Init_op=tf.global_variables_initializer() </span><br><span class="line">	sess_run(init_op)</span><br><span class="line">	STEPS=<span class="number">3000</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">		start=</span><br><span class="line">		end=</span><br><span class="line">		sess.run(train_step, feed_dict:)</span><br></pre></td></tr></table></figure></p>
<p>还是用前面的生产零件的例子来演示下这套八股。</p>
<p>随机产生 32 组生产出的零件的体积和重量，训练 3000 轮，每 500 轮输出一次损<br>失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"><span class="comment"># 第0步：导入模块，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line"><span class="comment"># 随机数种子固定，则输出结果固定</span></span><br><span class="line">seed = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment"># 随机数返回32行2列的矩阵，</span></span><br><span class="line"><span class="comment"># 表示32组体积和重量，作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从X这个32行2列的矩阵中，取出一行进行判断：</span></span><br><span class="line"><span class="comment"># 如果和小于1，Y赋值1，否则，Y赋值0。Y作为输入数据集的标签（即正确答案）</span></span><br><span class="line">Y = [[int(x0 + x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"X:\n"</span>, X</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Y:\n"</span>, Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第1步：定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2步：定义损失函数及后向传播过程</span></span><br><span class="line"><span class="comment"># 均方误差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line"><span class="comment"># 梯度下降，学习率为0.001</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment"># train_step = tf.train.MomentumOptimizer(0.001).minimize(loss)</span></span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第3步：生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#输出优化前的参数取值</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练模型</span></span><br><span class="line">    STEPS = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss, feed_dict=&#123;x: X, y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training step(s), loss on all data is %g"</span> % (i, total_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练后的参数取值</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">X:</span></span><br><span class="line"><span class="string">[[0.83494319 0.11482951]</span></span><br><span class="line"><span class="string"> [0.66899751 0.46594987]</span></span><br><span class="line"><span class="string"> [0.60181666 0.58838408]</span></span><br><span class="line"><span class="string"> [0.31836656 0.20502072]</span></span><br><span class="line"><span class="string"> [0.87043944 0.02679395]</span></span><br><span class="line"><span class="string"> [0.41539811 0.43938369]</span></span><br><span class="line"><span class="string"> [0.68635684 0.24833404]</span></span><br><span class="line"><span class="string"> [0.97315228 0.68541849]</span></span><br><span class="line"><span class="string"> [0.03081617 0.89479913]</span></span><br><span class="line"><span class="string"> [0.24665715 0.28584862]</span></span><br><span class="line"><span class="string"> [0.31375667 0.47718349]</span></span><br><span class="line"><span class="string"> [0.56689254 0.77079148]</span></span><br><span class="line"><span class="string"> [0.7321604  0.35828963]</span></span><br><span class="line"><span class="string"> [0.15724842 0.94294584]</span></span><br><span class="line"><span class="string"> [0.34933722 0.84634483]</span></span><br><span class="line"><span class="string"> [0.50304053 0.81299619]</span></span><br><span class="line"><span class="string"> [0.23869886 0.9895604 ]</span></span><br><span class="line"><span class="string"> [0.4636501  0.32531094]</span></span><br><span class="line"><span class="string"> [0.36510487 0.97365522]</span></span><br><span class="line"><span class="string"> [0.73350238 0.83833013]</span></span><br><span class="line"><span class="string"> [0.61810158 0.12580353]</span></span><br><span class="line"><span class="string"> [0.59274817 0.18779828]</span></span><br><span class="line"><span class="string"> [0.87150299 0.34679501]</span></span><br><span class="line"><span class="string"> [0.25883219 0.50002932]</span></span><br><span class="line"><span class="string"> [0.75690948 0.83429824]</span></span><br><span class="line"><span class="string"> [0.29316649 0.05646578]</span></span><br><span class="line"><span class="string"> [0.10409134 0.88235166]</span></span><br><span class="line"><span class="string"> [0.06727785 0.57784761]</span></span><br><span class="line"><span class="string"> [0.38492705 0.48384792]</span></span><br><span class="line"><span class="string"> [0.69234428 0.19687348]</span></span><br><span class="line"><span class="string"> [0.42783492 0.73416985]</span></span><br><span class="line"><span class="string"> [0.09696069 0.04883936]]</span></span><br><span class="line"><span class="string">Y:</span></span><br><span class="line"><span class="string">[[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]</span></span><br><span class="line"><span class="string">w1:</span></span><br><span class="line"><span class="string">[[-0.8113182   1.4845988   0.06532937]</span></span><br><span class="line"><span class="string"> [-2.4427042   0.0992484   0.5912243 ]]</span></span><br><span class="line"><span class="string">w2:</span></span><br><span class="line"><span class="string">[[-0.8113182 ]</span></span><br><span class="line"><span class="string"> [ 1.4845988 ]</span></span><br><span class="line"><span class="string"> [ 0.06532937]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">After 0 training step(s), loss on all data is 5.13118</span></span><br><span class="line"><span class="string">After 500 training step(s), loss on all data is 0.429111</span></span><br><span class="line"><span class="string">After 1000 training step(s), loss on all data is 0.409789</span></span><br><span class="line"><span class="string">After 1500 training step(s), loss on all data is 0.399923</span></span><br><span class="line"><span class="string">After 2000 training step(s), loss on all data is 0.394146</span></span><br><span class="line"><span class="string">After 2500 training step(s), loss on all data is 0.390597</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">w1:</span></span><br><span class="line"><span class="string">[[-0.7000663   0.9136318   0.08953571]</span></span><br><span class="line"><span class="string"> [-2.3402493  -0.14641267  0.58823055]]</span></span><br><span class="line"><span class="string">w2:</span></span><br><span class="line"><span class="string">[[-0.06024267]</span></span><br><span class="line"><span class="string"> [ 0.91956186]</span></span><br><span class="line"><span class="string"> [-0.0682071 ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>可以看到随着迭代次数的增加，损失函数loss的值越来越小。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/22/Spring集成MyBatis Generator/" rel="next" title="Spring集成MyBatis通用Mapper及Generator">
                <i class="fa fa-chevron-left"></i> Spring集成MyBatis通用Mapper及Generator
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Victor Chung" />
          <p class="site-author-name" itemprop="name">Victor Chung</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/VictorZhong" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-number">1.</span> <span class="nav-text"><a href="#&#x57FA;&#x672C;&#x6982;&#x5FF5;" class="headerlink" title="&#x57FA;&#x672C;&#x6982;&#x5FF5;"></a>&#x57FA;&#x672C;&#x6982;&#x5FF5;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#张量-Tensor-：多维数组-列表-，用“阶”表示其维度。"><span class="nav-number">1.1.</span> <span class="nav-text"><a href="#&#x5F20;&#x91CF;-Tensor-&#xFF1A;&#x591A;&#x7EF4;&#x6570;&#x7EC4;-&#x5217;&#x8868;-&#xFF0C;&#x7528;&#x201C;&#x9636;&#x201D;&#x8868;&#x793A;&#x5176;&#x7EF4;&#x5EA6;&#x3002;" class="headerlink" title="&#x5F20;&#x91CF;(Tensor)&#xFF1A;&#x591A;&#x7EF4;&#x6570;&#x7EC4;(&#x5217;&#x8868;)&#xFF0C;&#x7528;&#x201C;&#x9636;&#x201D;&#x8868;&#x793A;&#x5176;&#x7EF4;&#x5EA6;&#x3002;"></a>&#x5F20;&#x91CF;(Tensor)&#xFF1A;&#x591A;&#x7EF4;&#x6570;&#x7EC4;(&#x5217;&#x8868;)&#xFF0C;&#x7528;&#x201C;&#x9636;&#x201D;&#x8868;&#x793A;&#x5176;&#x7EF4;&#x5EA6;&#x3002;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算图-Graph-：搭建NN的计算过程，承载一个或多个计算节点（只搭建，不计算）"><span class="nav-number">1.2.</span> <span class="nav-text"><a href="#&#x8BA1;&#x7B97;&#x56FE;-Graph-&#xFF1A;&#x642D;&#x5EFA;NN&#x7684;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#xFF0C;&#x627F;&#x8F7D;&#x4E00;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x8BA1;&#x7B97;&#x8282;&#x70B9;&#xFF08;&#x53EA;&#x642D;&#x5EFA;&#xFF0C;&#x4E0D;&#x8BA1;&#x7B97;&#xFF09;" class="headerlink" title="&#x8BA1;&#x7B97;&#x56FE;(Graph)&#xFF1A;&#x642D;&#x5EFA;NN&#x7684;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#xFF0C;&#x627F;&#x8F7D;&#x4E00;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x8BA1;&#x7B97;&#x8282;&#x70B9;&#xFF08;&#x53EA;&#x642D;&#x5EFA;&#xFF0C;&#x4E0D;&#x8BA1;&#x7B97;&#xFF09;"></a>&#x8BA1;&#x7B97;&#x56FE;(Graph)&#xFF1A;&#x642D;&#x5EFA;NN&#x7684;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#xFF0C;&#x627F;&#x8F7D;&#x4E00;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x8BA1;&#x7B97;&#x8282;&#x70B9;&#xFF08;&#x53EA;&#x642D;&#x5EFA;&#xFF0C;&#x4E0D;&#x8BA1;&#x7B97;&#xFF09;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话-Session-：执行计算图的节点。"><span class="nav-number">1.3.</span> <span class="nav-text"><a href="#&#x4F1A;&#x8BDD;-Session-&#xFF1A;&#x6267;&#x884C;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x8282;&#x70B9;&#x3002;" class="headerlink" title="&#x4F1A;&#x8BDD;(Session)&#xFF1A;&#x6267;&#x884C;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x8282;&#x70B9;&#x3002;"></a>&#x4F1A;&#x8BDD;(Session)&#xFF1A;&#x6267;&#x884C;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x8282;&#x70B9;&#x3002;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NN的参数"><span class="nav-number">2.</span> <span class="nav-text"><a href="#NN&#x7684;&#x53C2;&#x6570;" class="headerlink" title="NN&#x7684;&#x53C2;&#x6570;"></a>NN&#x7684;&#x53C2;&#x6570;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络实现过程"><span class="nav-number">3.</span> <span class="nav-text"><a href="#&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;" class="headerlink" title="&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;"></a>&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">3.1.</span> <span class="nav-text"><a href="#&#x524D;&#x5411;&#x4F20;&#x64AD;" class="headerlink" title="&#x524D;&#x5411;&#x4F20;&#x64AD;"></a>&#x524D;&#x5411;&#x4F20;&#x64AD;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播TensorFlow描述"><span class="nav-number">3.2.</span> <span class="nav-text"><a href="#&#x524D;&#x5411;&#x4F20;&#x64AD;TensorFlow&#x63CF;&#x8FF0;" class="headerlink" title="&#x524D;&#x5411;&#x4F20;&#x64AD;TensorFlow&#x63CF;&#x8FF0;"></a>&#x524D;&#x5411;&#x4F20;&#x64AD;TensorFlow&#x63CF;&#x8FF0;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">3.3.</span> <span class="nav-text"><a href="#&#x53CD;&#x5411;&#x4F20;&#x64AD;" class="headerlink" title="&#x53CD;&#x5411;&#x4F20;&#x64AD;"></a>&#x53CD;&#x5411;&#x4F20;&#x64AD;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">3.3.1.</span> <span class="nav-text"><a href="#&#x635F;&#x5931;&#x51FD;&#x6570;" class="headerlink" title="&#x635F;&#x5931;&#x51FD;&#x6570;"></a>&#x635F;&#x5931;&#x51FD;&#x6570;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播的训练方法"><span class="nav-number">3.3.2.</span> <span class="nav-text"><a href="#&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x8BAD;&#x7EC3;&#x65B9;&#x6CD5;" class="headerlink" title="&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x8BAD;&#x7EC3;&#x65B9;&#x6CD5;"></a>&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x8BAD;&#x7EC3;&#x65B9;&#x6CD5;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#学习率"><span class="nav-number">3.3.3.</span> <span class="nav-text"><a href="#&#x5B66;&#x4E60;&#x7387;" class="headerlink" title="&#x5B66;&#x4E60;&#x7387;"></a>&#x5B66;&#x4E60;&#x7387;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建NN的八股"><span class="nav-number">4.</span> <span class="nav-text"><a href="#&#x642D;&#x5EFA;NN&#x7684;&#x516B;&#x80A1;" class="headerlink" title="&#x642D;&#x5EFA;NN&#x7684;&#x516B;&#x80A1;"></a>&#x642D;&#x5EFA;NN&#x7684;&#x516B;&#x80A1;</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Victor Chung</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
